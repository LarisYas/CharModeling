{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "import os\n",
    "import sys  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/yashunin/venvs/dl_venv/lib/python3.10/site-packages/debugpy/__init__.py'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.modules['debugpy'].__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/mysterious_island.txt', 'r', encoding='utf8') as file:\n",
    "    text = file.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1131711"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = text.find('THE MYSTERIOUS ISLAND')\n",
    "end_idx = text.find('End of the Project Gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text[start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1112350"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('&', 1), ('/', 1), ('=', 2), ('*', 3), ('(', 4)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter \n",
    "counter = Counter(text)\n",
    "print(len(set(text)))\n",
    "sorted(counter.items(), key=lambda arg: arg[1])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(set(text))\n",
    "char2int = { char : idx for idx, char in enumerate(chars) }\n",
    "int2char = np.array(chars)\n",
    "text_encoded = np.array([char2int[char] for char in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produced <==> [40 67 64 53 70 52 54 53]\n"
     ]
    }
   ],
   "source": [
    "print(f\"{text[30:38]} <==> {text_encoded[30:38]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader \n",
    "seq_size = 40\n",
    "chunk_size = seq_size + 1 \n",
    "chunks = torch.tensor(\n",
    "    np.array([text_encoded[i:chunk_size+i] for i in range(len(text_encoded) - chunk_size)])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_chunks):\n",
    "        self.text_chunks = text_chunks \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.text_chunks.size(0)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.text_chunks[idx]\n",
    "        return chunk[:-1], chunk[1:].to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_dataset = TextDataset(text_chunks=chunks)\n",
    "batch_size = 256\n",
    "torch.manual_seed(1)\n",
    "train_dl = DataLoader(\n",
    "    dataset=chunk_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=os.cpu_count(),\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([44, 32, 29,  1, 37, 48, 43, 44, 29, 42, 33, 39, 45, 43,  1, 33, 43, 36,\n",
       "         25, 38, 28,  1,  6,  6,  6,  0,  0,  0,  0,  0, 40, 67, 64, 53, 70, 52,\n",
       "         54, 53,  1, 51], dtype=torch.int32),\n",
       " tensor([32, 29,  1, 37, 48, 43, 44, 29, 42, 33, 39, 45, 43,  1, 33, 43, 36, 25,\n",
       "         38, 28,  1,  6,  6,  6,  0,  0,  0,  0,  0, 40, 67, 64, 53, 70, 52, 54,\n",
       "         53,  1, 51, 74]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim \n",
    "from torch.nn import functional as F \n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "\n",
    "class CharacterLevelModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=emb_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.classifier = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, inputs, hidden, cell):\n",
    "        inputs = self.embedding(inputs).unsqueeze(1)\n",
    "        out, (hidden, cell) = self.lstm(inputs, (hidden, cell))\n",
    "        out = self.classifier(out).view(out.size(0), -1)\n",
    "        return out, (hidden, cell)\n",
    "    \n",
    "    def init_hidden_cell(self, batch_size):\n",
    "        device = next(self.parameters()).device\n",
    "        init_hidden = torch.zeros(1, batch_size, self.hidden_size).to(device)\n",
    "        init_cell = torch.zeros(1, batch_size, self.hidden_size).to(device)\n",
    "        return init_hidden, init_cell \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(chars)\n",
    "emb_dim = 256\n",
    "hidden_size = 512 \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CharacterLevelModel(\n",
    "    vocab_size,\n",
    "    emb_dim=emb_dim,\n",
    "    hidden_size=hidden_size\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(\n",
    "    params=model.parameters(),\n",
    "    lr=1e-3\n",
    ")\n",
    "metric = Accuracy(task='multiclass', num_classes=vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "epochs = 2000\n",
    "torch.manual_seed(1)\n",
    "model.train() \n",
    "\n",
    "train_loop = tqdm(range(1, epochs + 1), desc='[Train]', leave=False)\n",
    "for epoch in train_loop:\n",
    "    optimizer.zero_grad() \n",
    "    \n",
    "    hidden, cell = model.init_hidden_cell(batch_size)\n",
    "    x_batch, y_batch = next(iter(train_dl))\n",
    "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_metric = 0 \n",
    "    for index in range(seq_size):\n",
    "        y_pred, (hidden, cell) = model(x_batch[:, index], hidden, cell)\n",
    "        epoch_loss += loss_fn(y_pred, y_batch[:, index])\n",
    "        epoch_metric += metric(y_pred, y_batch[:, index])\n",
    "    \n",
    "    epoch_loss /= seq_size\n",
    "    epoch_loss.backward() \n",
    "    optimizer.step()\n",
    "    \n",
    "    epoch_metric /= seq_size \n",
    "    \n",
    "    train_loop.set_description(f\"[Train, epoch {epoch}]: loss={epoch_loss:.3f}, metric={epoch_metric:.3f}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharacterLevelModel(\n",
       "  (embedding): Embedding(80, 256)\n",
       "  (lstm): LSTM(256, 512, batch_first=True)\n",
       "  (classifier): Linear(in_features=512, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3333, 0.3333, 0.3333]])\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions.categorical import Categorical \n",
    "torch.manual_seed(1)\n",
    "logits = torch.tensor([1., 1., 1.]).view(1, -1)\n",
    "print(F.softmax(logits, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = Categorical(logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding(torch.tensor([1, 2, 3])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, input_string, len_generated_text=1000, temperature=1.):\n",
    "    # batch size is equal to 1\n",
    "    text_encoded = torch.tensor(\n",
    "        [char2int[char] for char in input_string]\n",
    "    ).view(1, -1)\n",
    "    \n",
    "    generated_string = input_string\n",
    "    hidden, cell = model.init_hidden_cell(1)\n",
    "    \n",
    "    for index in range(len(input_string) - 1): \n",
    "        _, (hidden, cell) = model(text_encoded[:, index], hidden, cell)\n",
    "    \n",
    "    last_char = text_encoded[:, -1]\n",
    "    \n",
    "    for _ in range(len_generated_text):\n",
    "        logits, (hidden, cell) = model(last_char, hidden, cell)\n",
    "        logits = temperature * logits\n",
    "        sampler = Categorical(logits=logits)\n",
    "        last_char = sampler.sample()\n",
    "        generated_string += f\"{int2char[last_char]}\"\n",
    "    \n",
    "    return generated_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island was also that the sailor was a man who had been able to descend the point of the island was also that the colonists were the colonists were the colonists had been able to prove the sailor and the sailor and the sailor had not been able to say, the sailor was an apparatus which were then to be found the sailor and the sailor had not been able to prove the prisoners of the sailor’s hands.\n",
      "\n",
      "“Well, we will soon be seen that the colonists had not been a sort of six miles in the morning the colonists were the colonists were the colonists were the colonists were then to be found the colonists were the colonists were so much to the sailor and the sailor and the sailor had not been a sort of some day the sailor and the sailor was a man who was a man who had been seen that the sailor was always to be surprised the sailor and the sailor and the reporter and the reporter and the sailor and the reporter had not the sailor had not to be to be able to say the colonists were the colonists were the co\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, input_string='The island', temperature=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
